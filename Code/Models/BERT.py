# -*- coding: utf-8 -*-
"""bert-head-final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YKNjXqGw51kuHVaZdHPVf6NA1GRqSWPe
"""

import pandas as pd
import torch
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertForSequenceClassification
from transformers import AdamW
from tqdm import tqdm
import numpy as np
import os

# Check for GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load the dataset
dataset = "/kaggle/input/anlp-datasets/preprocessed data/news Headlines/"
train_dataset_file = dataset + "news_headlines_train.csv"
val_dataset_file = dataset + "news_headlines_dev.csv"

train_data = pd.read_csv(train_dataset_file)
val_data = pd.read_csv(val_dataset_file)

print("Dataset shape:", train_data.shape)
print("Column names:", train_data.columns)
print("\nSample data:")
print(train_data.head())
print("\n===================================================================================\n")

print("Dataset shape:", val_data.shape)
print("Column names:", val_data.columns)
print("\nSample data:")
print(val_data.head())

# Dataset class
class SarcasmDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_len):
        self.data = dataframe
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        text = self.data['preprocessed_text'].iloc[index]
        label = self.data['label'].iloc[index]

        encoding = self.tokenizer(
            text,
            max_length=self.max_len,
            padding="max_length",
            truncation=True,
            return_tensors="pt",
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(0),
            "attention_mask": encoding["attention_mask"].squeeze(0),
            "label": torch.tensor(label, dtype=torch.long),
        }

# Tokenizer and datasets
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
train_dataset = SarcasmDataset(train_data, tokenizer, max_len=128)
val_dataset = SarcasmDataset(val_data, tokenizer, max_len=128)

# DataLoaders
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)

# Model
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
model = model.to(device)

# Optimizer and Loss
optimizer = AdamW(model.parameters(), lr=2e-6)

# Training loop
def train_model(model, train_loader, val_loader, device, epochs=8, patience=3):
    best_loss = np.inf
    patience_counter = 0
    best_model_weights = None

    for epoch in range(epochs):
        print(f"\nEpoch {epoch + 1}/{epochs}")

        # Training phase
        model.train()
        train_loss = 0
        loop = tqdm(train_loader, leave=True)
        for batch in loop:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["label"].to(device)

            optimizer.zero_grad()
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            loop.set_description(f"Train loss: {loss.item():.4f}")

        avg_train_loss = train_loss / len(train_loader)

        # Validation phase
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                labels = batch["label"].to(device)

                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss
                val_loss += loss.item()

        avg_val_loss = val_loss / len(val_loader)
        print(f"Epoch {epoch + 1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}")

        # Early stopping
        if avg_val_loss < best_loss:
            best_loss = avg_val_loss
            patience_counter = 0
            best_model_weights = model.state_dict()  # Save best model weights
            torch.save(best_model_weights, "best_model_weights.pth")
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print("Early stopping triggered.")
            break

    return best_model_weights

best_model_weights = train_model(model, train_loader, val_loader, device)
print("Training complete. Best model saved as 'best_model_weights.pth'.")

# Load the test dataset
test_dataset_file = dataset + "news_headlines_test.csv"
test_data = pd.read_csv(test_dataset_file)

# Create a DataLoader for the test data
test_dataset = SarcasmDataset(test_data, tokenizer, max_len=128)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# Load the saved model weights
model.load_state_dict(torch.load("best_model_weights.pth"))
model = model.to(device)
model.eval()

# Necessary imports
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Function to evaluate the model on the test set
def evaluate_model_on_test_set(model, test_loader, device):
    predictions = []
    true_labels = []

    model.eval()  # Set the model to evaluation mode
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Evaluating"):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["label"].to(device)

            # Forward pass
            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits

            # Get predicted labels
            preds = torch.argmax(logits, dim=1)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())

    # Compute evaluation metrics
    accuracy = accuracy_score(true_labels, predictions)
    precision = precision_score(true_labels, predictions)
    recall = recall_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions)

    print(f"Test Set Performance:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")

    # Save evaluation metrics to CSV
    results = {
        "Metric": ["Accuracy", "Precision", "Recall", "F1 Score"],
        "Value": [accuracy, precision, recall, f1],
    }
    results_df = pd.DataFrame(results)
    results_df.to_csv("evaluation_results.csv", index=False)
    print(f"Results saved to evaluation_results.csv")

    return predictions

# Evaluate the model on the test set
predictions = evaluate_model_on_test_set(model, test_loader, device)

# Save test predictions to a new CSV file
output_file = "test_predictions.csv"
test_data["prediction"] = predictions
test_data[["preprocessed_text", "label", "prediction"]].to_csv(output_file, index=False)
print(f"Predictions saved to {output_file}")

# Load the saved predictions file
predictions_file = "test_predictions.csv"
predictions_df = pd.read_csv(predictions_file)

# Print a few random rows
print("Random rows from the test predictions file:")
print(predictions_df.sample(10))

# # Map values in 'label' and 'prediction' columns
# label_mapping = {0: "non-sarcastic", 1: "sarcastic"}
# predictions_df["label"] = predictions_df["label"].map(label_mapping)
# predictions_df["prediction"] = predictions_df["prediction"].map(label_mapping)

# # Save the updated file
# updated_file = "test_predictions_updated.csv"
# predictions_df.to_csv(updated_file, index=False)

# print(f"Updated predictions saved to {updated_file}")

# # Print a few random rows
# print("Random rows from the test predictions file:")
# print(predictions_df.sample(10))

